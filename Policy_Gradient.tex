\documentclass{article}
\usepackage{pedramnotes}

\title{Policy Gradient}
\author{Pedram Rabiee}
\setcounter{tocdepth}{2}

\begin{document}


\begin{titlepage}
\thispagestyle{empty}
\maketitle

\tableofcontents

Remaining:
From the Policy Gradient Lecture:
\begin{itemize}
    \item Policy Gradient Implementation
    \item Natural Policy Gradient
\end{itemize}
\end{titlepage}

\newgeometry{top=20mm,bottom=25mm,right=20mm,left=20mm}


\section{Introduction}
The policy gradient methods target at modeling and optimizing the policy directly. The policy is modeled with a parameterized function with respect to $\theta$
\begin{equation*}
    \pi(a \mid s,\theta) = \pi_\theta(a \mid s) = P\{A_t = a \mid S_t = s,\theta_t = \theta\}.
\end{equation*}
The objective is to maximize some performance measure $J(\theta)$
\begin{align*}
    \theta^* &= \argmax_\theta J(\theta)\\
    \theta &\gets \theta + \alpha \nabla_\theta J(\theta)
\end{align*}
To ensure exploration we generally require that the policy never becomes deterministic.

\subsection{Why Policy Gradient?}
\textbf{Advantages}
\begin{itemize}
    \item Better convergence properties.
    
    \item Effective in high-dimensional (there is no need to use maximization) or continuous action spaces.
    
    \item Can learn stochastic policies. In some problems, the optimal policy is stochastic policy (non-Markovian, aliased state). There is always an optimal deterministic policy for a given MDP, that's for Markov Decision Process where we have a perfect state representation. The moment you have state aliasing (so you are partially observed), you are in POMDP, or your function approximator, or the features that you use limit your view of the world which is equivalent to being in POMDP, then it can be optimal to use a stochastic policy, in which case policy-based method can do better than value based methods.
    
    \item For some problems policy is simpler than action-value function. For these problems a policy-based method will typically learn faster and yield a superior asymptotic policy.
    
    \item With value methods when you are using the max are extremely aggressive. This max in one step is trying to improve policy in direction that absolutely pushes you all the way to what currently think is the best policy. While policy gradient methods just take a little step in that direction, they smoothly update in that direction which makes them more stable, also sometimes less efficient, sometimes with high variance.
\end{itemize}
\textbf{Disadvantages}
\begin{itemize}
    \item Typically converges to a local rather than global optimum
    \item Evaluating a policy is typically inefficient and with high variance
\end{itemize}

\newpage
\section{Raw Policy Gradient}
\subsection{Definitions and Notations}
\begin{center}
\begin{tabular}{m{2cm} >{\arraybackslash}m{11cm}}
  \textbf{Symbol} & \textbf{Meaning} \\
  \hline
    $\rho^\pi(s\to x, k)$ & Probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\pi$\\
   \arrayrulecolor{gray}\hline
    $d^\pi(s)$ & Stationary probability of state $s$ under policy $\pi$\\    
    \arrayrulecolor{gray}\hline
    $p_0$ & Initial state distribution\\
    \arrayrulecolor{gray}\hline
    $G_t$ & Reward-to-go, starting from time $t$\\
    \arrayrulecolor{gray}\hline
    $\tau^{(t)}$ & Trajectory of $(s,a,r)$ tuples up to time $t$: the set $\{s_{t^\prime}, a_{t^\prime}, r_{t^\prime}\}_{t^\prime=0}^{t}$\\
    \arrayrulecolor{gray}\hline
    $p_\theta$ & Trajectory distribution parameterized by parameter $\theta$\\
    \arrayrulecolor{gray}\hline
    $w_{t_1\to t_2}$ & Product of importance sampling ratios from time $t_1$ to $t_2 \ge t_1$ (will be defined in~\Cref{sec:pg_off_policy})\\
\end{tabular}
\end{center}

Consider the following definitions
\begin{tcolorbox}[breakable,enhanced,colback=green!3!white,colframe=green!30!black]
\begin{align*}
    \rho^\pi(s\to x, k) &\triangleq \sum_{a_0} \pi(a_0|s) \sum_{s_1} P(s_1|s, a_0) \sum_{a_1} \pi(a_1|s_1) \sum_{s_2} P(s_2|s_1, a_1) \\
    &\phantom{\triangleq}\cdots \pi(a_{k-1}|s_{k-1}) P(x|s_{k-1}, a_{k-1}),\\
    \rho^\pi(s\to x, 0) & \triangleq \begin{cases} 
        %
        1,& \mbox{if } x = s, \\ 
        %
        0, & \mbox{else},
\end{cases}\\
    \eta^\pi(s) & \triangleq \sum_{s_0} p_0(s_0) \sum_{k=0}^\infty \rho^\pi(s_0\to s, k).
\end{align*}
\end{tcolorbox}

Then, we have
\begin{align*}
    \rho^\pi(s\to x, 1) &= \sum_{a_0} \pi(a_0|s) P(x|s, a_0),\\
    \rho^\pi(s\to x, k+1) &= \sum_{s^\prime}\rho^\pi(s\to s^\prime, k) \rho^\pi(s^\prime\to x, 1).
\end{align*}
\begin{tcolorbox}[breakable,enhanced,colback=green!3!white,colframe=green!30!black]
The stationary distribution $d^\pi$ is defined as
\begin{align*}
    d^\pi(s) \triangleq \frac{\eta^\pi(s)}{\sum_s \eta^\pi(s)}.
\end{align*}

Also, consider reward-to-go $G_t$ defined as
\begin{align*}
    G_t \triangleq \sum_{t^\prime= t}^{T-1} r(s_{t^\prime}, a_{t^\prime}).
\end{align*}

For the trajectory $\tau^{(t)}\triangleq (s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_t, a_t, r_t)$, the trajectory distribution $p_\theta$ is defined as

\begin{align*}
    p_\theta(\tau^{(t)}) \triangleq p_0(s_0)\prod_{t^\prime = 0}^{t-1} \pi_\theta(a_{t^\prime}|s_{t^\prime})P(s_{t^\prime+1}|s_{t^\prime}, a_{t^\prime}) \pi_\theta(a_t|s_t)
\end{align*}

Note that for the case where $t = T$, where $T$ is the length of an episode, we may use $\tau$ to refer to $\tau^{(T-1)}$.

\end{tcolorbox}


\subsection{Policy Gradient Derivation}\label{sec:pg_derivation}

Starting from different objectives, we obtain different policy gradient expressions.\Cref{table:pg_exp} lists two objectives and their corresponding policy gradient expressions.
\begin{table}[H]
    \centering
\begin{tabular}{c c}
  \textbf{Objective:} $J(\theta)$ & \textbf{Objective's Gradient:} $\nabla_\theta J(\theta)$ \\
  \hline
   $\BBE_{s_0\sim p_0}\left[V^{\pi_\theta}(s_0)\right]$ & $\BBE_{a \sim\pi _\theta, s\sim d^{\pi_\theta}}\left[Q^{\pi_\theta}(s,a)\nabla_\theta \log \pi_\theta(a|s)\right]$ \\
   \arrayrulecolor{gray}\hline
    $\sum_{t=0}^{T-1} \BBE_{\tau^{(t)} \sim p_\theta}[r(s_t, a_t)]$ & $\BBE_{\tau^{(T-1)} \sim p_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) G_t\right]$ \\  
\end{tabular}
\caption{Policy gradient objective and expressions}
\label{table:pg_exp}
\end{table}

Now, consider the first objective $J(\theta) =\BBE_{s_0\sim p_0}\left[V^{\pi_\theta}(s_0)\right]$. We put forward the proof from~\cite{sutton2018reinforcement}. It is to be noted that, the subscript $\theta$ is often times dropped from $\pi$ in the proofs to save space.
\begin{tcolorbox}[breakable,enhanced,colback=gray!10!white,colframe=gray!50!black,
title={Policy Gradient Derivation for $J(\theta) = \BBE_{s_0\sim p_0}\left[V^{\pi_\theta}(s_0)\right]$}]

We first seek to extend the expression for $\nabla_\theta V^\pi(s)$, for $s\in \SSS$.
\begin{align*}
    \nabla_\theta V^{\pi}(s) &= \nabla_\theta\left(\sum_a \pi_\theta(a|s)Q^\pi(s,a)\right)\\
    % 
    &=\sum_a\left(\nabla_\theta \pi_\theta(a|s)Q^\pi(s,a) + \pi_\theta(a|s)\nabla_\theta Q^\pi(s,a)\right) \\
    % 
    &= \sum_a\left(\nabla_\theta \pi_\theta(a|s)Q^\pi(s,a) + \pi_\theta(a|s)\nabla_\theta \sum_{s^\prime}P(s^\prime|s,a)[r(s,a) + V^\pi(s^\prime)]\right)\\
    % 
    &=\sum_a\left(\nabla_\theta \pi_\theta(a|s)Q^\pi(s,a) + \pi_\theta(a|s) \sum_{s^\prime}P(s^\prime|s,a) \nabla_\theta V^\pi(s^\prime)\right)\\
    % 
    &=\underbrace{\sum_a\nabla_\theta \pi_\theta(a|s)Q^\pi(s,a)}_{\triangleq \phi(s)} + \sum_a\pi_\theta(a|s) \sum_{s^\prime}P(s^\prime|s,a) \nabla_\theta V^\pi(s^\prime)\\
    % 
    &=\phi(s) + \sum_{s^\prime}\rho(s\to s^\prime,1) \nabla_\theta V^\pi(s^\prime)\\
    % 
    &=\phi(s) + \sum_{s^\prime}\rho(s\to s^\prime,1) \left[ \phi(s^\prime) +\sum_{s^{\dprime}}\rho(s^\prime\to s^{\dprime},1)\nabla_\theta V^\pi(s^{\dprime})\right]\\
    % 
    &=\phi(s) + \sum_{s^\prime}\rho(s\to s^\prime,1) \phi(s^\prime) +\sum_{s^\prime}\rho(s\to s^\prime,1)\sum_{s^{\dprime}}\rho(s^\prime\to s^{\dprime},1)\nabla_\theta V^\pi(s^{\dprime})\\
    % 
    &=\phi(s) + \sum_{s^\prime}\rho(s\to s^\prime,1) \phi(s^\prime) +\sum_{s^{\dprime}}\rho(s\to s^{\dprime},2)\nabla_\theta V^\pi(s^{\dprime})\\
    % 
    &=\phi(s) + \sum_{s^\prime}\rho(s\to s^\prime,1) \phi(s^\prime) +\sum_{s^{\dprime}}\rho(s\to s^{\dprime},2)\phi(s^{\dprime})\\    
    &\phantom{=\phi(s) + \sum_{s^\prime}\rho(s\to s^\prime,1) \phi(s^\prime)~}+\sum_{s^{\tprime}}\rho(s\to s^{\tprime},3)\nabla_\theta V^\pi(s^{\tprime})\\
    % 
    &=\sum_{x\in\SSS} \sum_{k=0}^\infty \rho^\pi(s\to x, k)\phi(x).
\end{align*}
Now, we derive the expression for $\nabla_\theta J(\theta)$.
\begin{align*}
    \nabla_\theta J(\theta) &= \nabla_\theta \BBE_{s_0\sim p_0}\left[V^{\pi}(s_0)\right]\\    
    &=\sum_{s_0}p_0(s_0)\sum_{s\in\SSS}\sum_{k=0}^\infty \rho^\pi(s_0\to s, k)\phi(x) \\
    &=\sum_{s\in\SSS} \phi(s)\underbrace{\sum_{s_0}p_0(s_0)\sum_{k=0}^\infty \rho^\pi(s_0\to s, k)}_{=\eta^\pi(s)} \\
    &= \sum_{s}\eta^\pi(s)\phi(s)\\
    &=\underbrace{\sum_{s^\prime}\eta^\pi(s^\prime)}_{=1 \text{ for continuing case}} \sum_{s}\frac{\eta^\pi(s)}{\sum_{s^\prime}\eta^\pi(s^\prime)}\phi(s)\\
    &=\sum_{s}d^\pi(s)\phi(s)\\
    &=\sum_{s}d^\pi(s)\sum_a \nabla_\theta\pi_\theta(a|s) Q^\pi(s,a)\\
    &=\sum_{s}d^\pi(s)\sum_a  Q^\pi(s,a)\pi_\theta(a|s)\nabla_\theta\log\pi_\theta(a|s)\\
    &=\BBE_{a \sim\pi_\theta, s\sim d^{\pi_\theta}} \left[Q^{\pi_\theta}(s,a)\nabla_\theta\log\pi_\theta(a|s)\right]\\
\end{align*}
\end{tcolorbox}

What the expression $\nabla J(\theta) = \BBE_{a \sim\pi_\theta, s\sim d^{\pi_\theta}} \left[Q^{\pi_\theta}(s,a)\nabla_\theta\log\pi_\theta(a|s)\right]$ asserts is that to improve the policy $\pi_\theta$, take the average of $Q^{\pi_\theta}(s,a)\nabla_\theta\log\pi_\theta(a|s)$, for all state and action pairs $(s, a)$, where the state is sampled from the stationary distribution $d^{\pi_\theta}$, and the action is sampled from the policy $\pi_\theta$.

Now, let's consider the second objective $J(\theta) = \sum_{t=0}^{T-1} \BBE_{\tau^{(t)} \sim p_\theta}[r(s_t, a_t)]$.
\begin{tcolorbox}[breakable,enhanced,colback=gray!10!white,colframe=gray!50!black,
title={Policy Gradient Derivation for $J(\theta) = \sum_{t=0}^{T-1} \BBE_{\tau^{(t)} \sim p_\theta}[r(s_t, a_t)]$}]

\begin{align*}
\nabla_\theta J(\theta) &= \nabla_\theta \sum_{t=0}^{T-1} \BBE_{\tau^{(t)} \sim p_\theta}[r(s_t, a_t)] \\
&= \sum_{t=0}^{T-1} \nabla_\theta \BBE_{\tau^{(t)} \sim p_\theta}[r(s_t, a_t)] \\
&= \sum_{t=0}^{T-1} \int \nabla_\theta \left(p_\theta(\tau^{(t)}) r(s_t, a_t)\right) \dtau^{(t)} \\
&= \sum_{t=0}^{T-1} \int p_\theta(\tau^{(t)}) \nabla_\theta \log p_\theta(\tau^{(t)}) r(s_t, a_t) \dtau^{(t)} \\
&= \sum_{t=0}^{T-1} \BBE_{\tau^{(t)} \sim p_\theta} \left[\nabla_\theta \log p_\theta(\tau^{(t)}) r(s_t, a_t)\right].
\end{align*}
Now, first take a look at $\nabla_\theta \log p_\theta(\tau^{(t)})$
\begin{align*}
\nabla_\theta \log p_\theta(\tau^{(t)}) &= \nabla_\theta \log \left( p_0(s_0)\prod_{t^\prime = 0}^{t-1} \pi_\theta(a_{t^\prime}|s_{t^\prime})P(s_{t^\prime+1}|s_{t^\prime}, a_{t^\prime}) \pi_\theta(a_t|s_t)\right)\\
&= \nabla_\theta \left(\log p_0(s_0)+\sum_{t^\prime = 0}^{t-1} \log \pi_\theta(a_{t^\prime}|s_{t^\prime}) + \log P(s_{t^\prime+1}|s_{t^\prime}, a_{t^\prime}) + \log \pi_\theta(a_t|s_t)\right)\\
&= \sum_{t^\prime = 0}^{t} \nabla_\theta \log \pi_\theta(a_{t^\prime}|s_{t^\prime}).
\end{align*}
Then, we have
\begin{align*}
    \nabla_\theta J(\theta) &= \sum_{t=0}^{T-1} \BBE_{\tau^{(t)} \sim p_\theta}  \underbrace{\left[r(s_t, a_t)\sum_{t^\prime = 0}^{t} \nabla_\theta \log \pi_\theta(a_{t^\prime}|s_{t^\prime})\right]}_{\triangleq g_t}
\end{align*}
Now, we can take the summation inside the expectation by writing all expectations under$p_\theta(\tau^{(T-1)})$. In other words, $\sum_{t=0}^{T-1} \BBE_{\tau^{(t)} \sim p_\theta}[g_t] = \BBE_{\tau^{(T-1)} \sim p_\theta}\left[\sum_{t=0}^{T-1} g_t\right]$. We will demonstrate why this is correct, later. Thus, we have

\begin{align*}
    \nabla_\theta J(\theta) = \BBE_{\tau^{(T-1)} \sim p_\theta} \left[ \sum_{t=0}^{T-1} r(s_t,a_t) \sum_{t'=0}^{t} \nabla_\theta \log \pi_\theta(a_{t'} \vert s_{t'}) \right]
\end{align*}

Using an algebraic trick, we can rearrange this expression as follows:

Let $f_t \triangleq \nabla_\theta \log \pi_\theta(a_{t'} \vert s_{t'})$ and $r_t \triangleq r(s_t,a_t)$. We expand the expression inside the expectation in the following form:
\begin{align*}
&r_0 f_0 +\\
&r_1 f_0 + r_1 f_1 +\\
&r_2 f_0 + r_2 f_1 + r_2 f_2 +\\
&\ldots \\
&r_{T-1} f_0 + r_{T-1} f_1 + \ldots + r_{T-1} f_{T-1}
\end{align*}
Now, instead of summing row-wise, we sum column-wise:
\begin{align*}
(r_0 + r_1 + \ldots + r_{T-1}) f_0 + (r_1 + \ldots + r_{T-1}) f_1 + (r_2 + \ldots + r_{T-1}) f_2 + \ldots + r_{T-1} f_{T-1}   
\end{align*}

Thus, we have:

\begin{align*}
\nabla_\theta J(\theta) = \BBE_{\tau^{(T-1)} \sim p_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) \sum_{t'=t}^{T-1} r(s_t,a_t) \right] = \BBE_{\tau^{(T-1)} \sim p_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) G_t \right]
\end{align*}
This concludes the proof, except for the fact that we need to show that:
\begin{align}\label{eq:traj_exp}
    \sum_{t=0}^{T-1} \BBE_{\tau^{(t)} \sim p_\theta}[g_t] = \BBE_{\tau^{(T-1)} \sim p_\theta}\left[\sum_{t=0}^{T-1} g_t\right],
\end{align}
where $g_t \triangleq r(s_t, a_t) \sum_{t'=0}^{t} \nabla_\theta \log \pi_\theta(a_{t'} \vert s_{t'})$.
We have
\begin{align*}
\BBE_{\tau^{(T-1)} \sim p_\theta} \left[\sum_{t=0}^{T-1} g_t\right] 
&= \BBE_{\tau^{(T-1)} \sim p_\theta} \left[g_0 + g_1 + \ldots + g_t + \ldots + g_{T-1}\right]\\
&=\sum_{t=0}^{T-1}\BBE_{\tau^{(T-1)} \sim p_\theta} [g_t]
\end{align*}

Thus, in order to prove \eqref{eq:traj_exp}, it is sufficient to show that for arbitrary $t$, $\BBE_{\tau^{(t)} \sim p_\theta}[g_t] = \BBE_{\tau^{(T-1)} \sim p_\theta}[ g_t]$.

\begin{align*}
\BBE_{\tau^{(T-1)} \sim p_\theta} [g_t]&= \int_{s_0} \int_{a_0} \ldots \int_{s_t} \int_{a_t} \ldots \int_{s_T} p_0(s_0) \pi_\theta(a_0 \vert s_0) \ldots P(s_t \vert s_{t-1}, a_{t-1}) \pi_\theta(a_t \vert s_t) g_t \ds_0 \da_0 \ldots \ds_t \da_t \ldots \ds_T \\
% 
&= \int_{s_0} \int_{a_0} \ldots \int_{s_t} \int_{a_t} p_0(s_0) \pi_\theta(a_0 \vert s_0) \ldots P(s_t \vert s_{t-1}, a_{t-1}) \pi_\theta(a_t \vert s_t) g_t  \\
&\qquad\qquad\int_{s_{t+1}}\ldots \int_{a_{T-1}} \pi_\theta(a_{T-1} \vert s_{T-1}) \, \da_{T-1} \ldots \da_0 \ds_0 \\
% 
&= \int_{s_0} \int_{a_0} \ldots \int_{s_t} \int_{a_t} p_0(s_0) \pi_\theta(a_0 \vert s_0) \ldots P(s_t \vert s_{t-1}, a_{t-1}) \pi_\theta(a_t \vert s_t) g_t\\
&\qquad \qquad\int_{s_{t+1}} \ldots \int_{a_{T-1}} \pi_\theta(a_{T-1} \vert s_{T-1}) \, \da_{T-1} \ldots \da_0 \ds_0 \\
% 
&= \int p_\theta(\tau^{(t)}) g_t \, \dtau^{(t)} = \BBE_{\tau^{(t)} \sim p_\theta}[g_t]
\end{align*}

This concludes the result.

\end{tcolorbox}


\begin{tcolorbox}[colback=blue!10!white,colframe=blue!50!black]
\textbf{Important note:}
The version of the policy gradient described in this section is on-policy since the expectations are under current policy. Thus, you can't use samples that come from other policies, and samples collected under the current policy have to be thrown away after each update. This makes this method extremely inefficient.
\end{tcolorbox}


\subsection{REINFORCE}
\begin{figure}[ht]
\center{\includegraphics[width=0.7\textwidth] {Reinforce}}
\caption{REINFORCE Algorithm}\label{fig:reinforce}
\end{figure} 

\subsection{State-Based Baseline}
In order to reduce variance, we introduce a baseline to our gradient estimate. The addition of a state-based baseline $b(s)$ does not introduce bias into our estimate. That is, the gradient of the policy objective function $J(\theta)$ can be expressed as:
\begin{align*}
    \nabla_\theta J(\theta) = \BBE_{\tau \sim p_\theta} \left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) G_t\right] = \BBE_{\tau \sim p_\theta} \left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) (G_t - b(s_t))\right]
\end{align*}

In other words, $\BBE_{\tau \sim p_\theta} \left[\nabla_\theta \log \pi_\theta(a_t \vert s_t) b(s_t)\right] = 0$ for any $t$.


Before proving this we first show that
\begin{equation}\label{eq:baseline_proof_lemma}
    \int_{a_t} \pi_\theta(a_t \vert s_t) \nabla_\theta \log \pi_\theta(a_t \vert s_t) \, \da_t = 0
\end{equation}

\begin{tcolorbox}[breakable,enhanced,colback=gray!10!white,colframe=gray!50!black,
title={}]
\begin{align*}
\int_{a_t} \pi_\theta(a_t \vert s_t) \nabla_\theta \log \pi_\theta(a_t \vert s_t) \, \da_t &= \int_{a_t} \pi_\theta(a_t \vert s_t) \left(\frac{\nabla_\theta \pi_\theta(a_t \vert s_t)}{\pi_\theta(a_t \vert s_t)}\right) \, \da_t \\
&= \int_{a_t} \nabla_\theta \pi_\theta(a_t \vert s_t) \, \da_t \\
&= \nabla_\theta \int_{a_t} \pi_\theta(a_t \vert s_t) \, \da_t = \nabla_\theta \cdot 1 = 0
\end{align*}
\end{tcolorbox}
Next, consider the proof of $\BBE_{\tau \sim p_\theta} \left[\nabla_\theta \log \pi_\theta(a_t \vert s_t) b(s_t)\right] = 0$.
\begin{tcolorbox}[breakable,enhanced,colback=gray!10!white,colframe=gray!50!black,
title={$\BBE_{\tau \sim p_\theta} \left[\nabla_\theta \log \pi_\theta(a_t \vert s_t) b(s_t)\right] = 0$}]
\begin{align*}
&\BBE_{\tau \sim p_\theta} \left[\nabla_\theta \log \pi_\theta(a_t \vert s_t) b(s_t)\right] \\
%
&= \int p_\theta(\tau) \nabla_\theta \log \pi_\theta(a_t \vert s_t) b(s_t) \dtau \\
%
&= \int_{s_0} \int_{a_0} \cdots \int_{s_t} \int_{a_t} \cdots \int_{s_T} p_0(s_0) \pi_\theta(a_0 \vert s_0) \cdots P(s_t \vert s_{t-1}, a_{t-1}) \pi_\theta(a_t \vert s_t) \cdots P(s_T \vert s_{T-1}, a_{T-1}) \\
&\qquad \qquad \qquad \nabla_\theta \log \pi_\theta(a_t \vert s_t) b(s_t) \, \ds_0 \, \da_0 \cdots \ds_T \\
%
&= \int_{s_0} \cdots \int_{s_t} \int_{a_t} p_0(s_0) \cdots P(s_t \vert s_{t-1}, a_{t-1}) \pi_\theta(a_t \vert s_t) \nabla_\theta \log \pi_\theta(a_t \vert s_t) b(s_t) \\
&\qquad \qquad \qquad \cdots \int_{a_{T-1}} \pi_\theta(a_{T-1} \vert s_{T-1}) \underbrace{\int_{s_T} P(s_T \vert s_{T-1}, a_{T-1})\,\ds_T}_{=1} \, \da_{T-1} \cdots \, \da_0 \, \ds_0 \\
% 
&= \int_{s_0} \cdots \int_{s_t} \int_{a_t} p_0(s_0) \cdots P(s_t \vert s_{t-1}, a_{t-1}) \pi_\theta(a_t \vert s_t) \nabla_\theta \log \pi_\theta(a_t \vert s_t) b(s_t) \\
&\qquad \qquad \qquad \cdots  \underbrace{\int_{a_{T-1}} \pi_\theta(a_{T-1} \vert s_{T-1}) \, \da_{T-1}}_{=1} \,  \cdots \, \da_0 \, \ds_0 \\
% 
&= \int_{s_0} \int_{a_0} \cdots \int_{s_t} \int_{a_t} p_0(s_0) \pi_\theta(a_0 \vert s_0) \cdots P(s_t \vert s_{t-1}, a_{t-1}) \pi_\theta(a_t \vert s_t) \\
&\qquad \qquad \qquad \nabla_\theta \log \pi_\theta(a_t \vert s_t) b(s_t) \, \da_t \, \ds_t \cdots \da_0 \, \ds_0 \\
% 
&= \int_{s_0} \int_{a_0} \cdots \int_{s_t} p_0(s_0) \pi_\theta(a_0 \vert s_0) \cdots P(s_t \vert s_{t-1}, a_{t-1}) b(s_t) \\
&\qquad \qquad \qquad \underbrace{\int_{a_t} \pi_\theta(a_t \vert s_t) \nabla_\theta \log \pi_\theta(a_t \vert s_t) \, \da_t}_{=0 \text{ from~\eqref{eq:baseline_proof_lemma}}} \, \ds_t \cdots \da_0 \, \ds_0 \\
&=0
\end{align*}
\end{tcolorbox}
Thus, we have shown that \(\BBE_{\tau \sim p_\theta} \left[\nabla_\theta \log \pi_\theta(a_t \vert s_t) b(s_t)\right] = 0 \).

But why does it reduce the variance? Intuitively, by making the target values smaller (by subtracting the baseline), we are reducing the variance. To understand why the introduction of a baseline reduces variance, we consider informal reasoning by finding the baseline that minimizes the variance.

The variance \(\text{Var}(X)\) of a random variable \(X\) is defined as

\begin{align*}
\text{Var}(X) \triangleq \BBE[X^2] - \BBE[X]^2.
\end{align*}

We want to show that $\text{Var}(X) < \text{Var}(Y)$, where $X = \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) (G_t - b(s_t))   $, and $Y = \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) G_t$.

\vspace{10pt}
\textbf{Analysis 1. Finding the baseline that minimizes the variance} 


Let $X = \underbrace{\nabla_\theta \log \pi_\theta(a_t \vert s_t)}_{\triangleq \psi_t} (G_t - \underbrace{b(s_t)}_{\triangleq b_t})$

Now, let's consider the variance:
\begin{align*}
    \text{Var}(\psi_t (G_t - b_t)) = \BBE[(\psi_t (G_t - b_t))^2] - \BBE[\psi_t (G_t - b_t)]^2
\end{align*}

We showed that $\BBE[\psi_t (G_t - b_t)] = \BBE[\psi_t (G_t)]$, thus, the second term, \(\BBE[g_t (G_t - b_t)]^2\), does not depend on the choice of \(b_t\).
In order to minimize the variance, we need to find the optimal baseline \(b(s_t)\) that satisfies the condition \(\frac{d\text{Var}(\psi_t(G_t - b_t)}{db_t} = 0\). Differentiating with respect to \(b_t\) and setting it to zero, we obtain:

\[
\frac{d\text{Var}(\psi_t (G_t - b_t))}{db_t} = \frac{d}{db_t} \BBE[(\psi_t (G_t - b_t))^2] = \BBE[-2\psi_t^2 (G_t - b_t)] = 0
\]

Simplifying the equation, we find that:
\[
\BBE[\psi_t^2 G_t] = \BBE[\psi_t^2 b_t]
\]
This allows us to determine the optimal baseline \(b_t\) as:

\[
b_t = \frac{\BBE[(\nabla_\theta \log \pi_\theta(a_t \vert s_t))^2 G_t]}{\BBE[(\nabla_\theta \log \pi_\theta(a_t \vert s_t))^2]}
\]

While this expression provides a general formula for the optimal baseline, in practice, a common choice is to use the on-policy value function \(V_\pi(s_t)\) as the baseline.

\begin{itemize}
    \item Another informal reasoning can be found 
\href{https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#:~:text=The%20variance%20is%20approximated%20as%3A
}{\underline{here}}.
    \item For a discussion on a more general form of a baseline see~\cref{sec:GAE}.
\end{itemize}


\subsection{Vanilla Policy Gradient}

\begin{figure}[ht]
\center{\includegraphics[width=0.7\textwidth] {Vanilla}}
\caption{Vanilla Policy Gradient Algorithm}\label{fig:vanilla}
\end{figure} 

\subsection{Off-Policy Policy Gradient Using Importance Sampling}\label{sec:pg_off_policy}
Assume that instead of having samples collected under $\pi_\theta$, we have samples collected under another policy $\pi_\beta$. In this case, the policy gradient can be expressed in terms of the expectation over the trajectories under $\pi_\beta$ using importance sampling. First, for $t_2 \ge t_1$, define $w_{t_1 \to t_2}$ as
\begin{align}
w_{t_1 \to t_2} \triangleq \prod_{t= t_1}^{t_2}
    \frac{\pi_\theta(a_t \vert s_t)}{\pi_\beta(a_t \vert s_t)}.
\end{align}
Next, let $r(\tau)$ denote the sum of reward for trajectory $\tau$, then
\begin{align*}
    J(\theta) = \BBE_{\tau \sim p_\theta} [r(\tau)] = \BBE_{\tau \sim p_\beta} \left[\frac{p_\theta(\tau)}{p_\beta(\tau)} r(\tau) \right]
\end{align*}
where 
\begin{align*}
    \frac{p_\theta}{p_\beta} = \frac{p_0(s_0) \prod_{t=0}^{T-1}\pi_\theta(a_t \vert s_t) P(s_{t+1}\vert a_t, s_t)}{p_0(s_0) \prod_{t=0}^{T-1}\pi_\beta(a_t \vert s_t) P(s_{t+1}\vert a_t, s_t)} =
    \prod_{t=0}^{T-1}
    \frac{\pi_\theta(a_t \vert s_t)}{\pi_\beta(a_t \vert s_t)} = w_{0\to{T-1}}.
\end{align*}

Thus,
\begin{align}\label{eq:off_policy_pg_raw}
    \nabla_\theta J(\theta) = \BBE_{\tau \sim p_\beta} \left[w_{0 \to {T-1}} \left(\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t\vert s_t)\right) \left(\sum_{t=0}^{T-1} r(s_t, a_t)\right) \right]
\end{align}
This version of off-policy policy gradient however have high variance.
From here forward, different works, make different estimations of~\eqref{eq:off_policy_pg_raw} in order to reduce the variance.

Causality trick 1: Current action cannot affect the past rewards. Thus, we change the trajectory reward to reward-to-go.

\begin{align*}
    \nabla_\theta J(\theta) &\approx \BBE_{\tau \sim p_\beta} \left[w_{0 \to {T-1}} \left(\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t\vert s_t) \sum_{t^\prime=t}^T r(s_{t^\prime}, a_{t^\prime})\right) \right] \\
    &= \BBE_{\tau \sim p_\beta} \left[\left(\sum_{t=0}^{T-1} w_{0 \to {T-1}} \nabla_\theta \log \pi_\theta(a_t\vert s_t) \sum_{t^\prime=t}^{T-1} r(s_{t^\prime}, a_{t^\prime})\right) \right]\\
    &= \BBE_{\tau \sim p_\beta} \left[\left(\sum_{t=0}^{T-1} w_{0\to t} \nabla_\theta \log \pi_\theta(a_t\vert s_t) w_{{t+1} \to {T-1}} \sum_{t^\prime=t}^{T-1} r(s_{t^\prime}, a_{t^\prime})\right) \right]
    % &= \BBE_{\tau \sim p_\beta} \left[\left(\sum_{t=0}^{T-1} \left( \prod_{t^\prime=0}^{t}\frac{\pi_\theta(a_{t^\prime} \vert s_{t^\prime})}{\pi_\beta(a_{t^\prime} \vert s_{t^\prime})} \right) \nabla_\theta \log \pi_\theta(a_t\vert s_t) \left( \prod_{t^\prime=t+1}^{T-1}\frac{\pi_\theta(a_{t^\prime} \vert s_{t^\prime})}{\pi_\beta(a_{t^\prime} \vert s_{t^\prime})} \right) \sum_{t^\prime=t}^{T-1} r(s_{t^\prime}, a_{t^\prime})\right) \right]
\end{align*}

Causality trick 2: Future actions cannot affect the current reward. Thus, we drop the importance samplings into the future. First note that
\begin{align*}
    w_{{t+1} \to {T-1}} = \frac{w_{{0} \to {T-1}}}{w_{{0} \to {t}}}
\end{align*}

\begin{align}
\nabla_\theta J(\theta) &\approx \BBE_{\tau \sim p_\beta} \left[\left(\sum_{t=0}^{T-1} w_{0\to t} \nabla_\theta \log \pi_\theta(a_t\vert s_t)  \sum_{t^\prime=t}^{T-1} \frac{w_{{0} \to {T-1}}}{w_{{0} \to {t}}} r(s_{t^\prime}, a_{t^\prime})\right) \right]\nn\\
&\approx \BBE_{\tau \sim p_\beta} \left[\left(\sum_{t=0}^{T-1} w_{0\to t} \nabla_\theta \log \pi_\theta(a_t\vert s_t)  \sum_{t^\prime=t}^{T-1} \frac{w_{{0} \to {t^\prime}}}{w_{{0} \to {t}}} r(s_{t^\prime}, a_{t^\prime})\right) \right] \label{eq:off_policy_pg_after_causality1}
\end{align}

Introducing state-based baseline and discount factor to~\eqref{eq:off_policy_pg_after_causality1} yields
\begin{align}
\nabla_\theta J(\theta) &\approx \BBE_{\tau \sim p_\beta} \left[\left(\sum_{t=0}^{T-1} w_{0\to t}\gamma^t \nabla_\theta \log \pi_\theta(a_t\vert s_t)  \left(\sum_{t^\prime=t}^{T-1} \gamma^{t^\prime - t}\frac{w_{{0} \to {t^\prime}}}{w_{{0} \to {t}}} r(s_{t^\prime}, a_{t^\prime}) - b(s_t)\right)\right) \right],\label{eq:off_policy_pg_after_causality2}
\end{align}
which is the formulation in~\cite{precup2000eligibility}.

As before,~\eqref{eq:off_policy_pg_after_causality2} can have high variance due to having the multiplication of importance sampling fractions.
One approach to further reduce the variance is to make sure that the learned policy $\pi_\theta$ does not deviate too far from the behavior policy $\pi_\beta$, thus keeping the variance of the importance weights from becoming too large~\cite{levine2020offline}.
One way to achive this to introduce regulizer~\cite{levine2013guided,levine2020offline}:
\begin{align*}
\nabla_\theta J(\theta) &\approx \BBE_{\tau \sim p_\beta} \left[\left( w_{{0} \to {T-1}} \sum_{t=0}^{T-1}\gamma^t \nabla_\theta \log \pi_\theta (a_t\vert s_t) \hat A(s_t,a_t)\right) + \lambda \log w_{{0} \to {T-1}}\right].
\end{align*}
This regulizer which is the softmax over the importance weight, adjust the policy $\pi_\theta$ to ensure that at least one sample has a high importance weight.

\subsection{Policy Gradient Implementation Notes}

\begin{enumerate}
    \item \textcolor{red}{with Automatic Differentiation Tools}
    \item \textcolor{red}{On normalization}
\end{enumerate}

\newpage
\section{Actor-Critic}

To reduce variance in policy gradient methods, one approach is to employ a bootstrapping target. By replacing the advantage function used in the Vanilla policy gradient with a TD(0)-like target, we arrive at the actor-critic algorithm. This method mitigates variance but introduces bias. Much like the Vanilla policy gradient, the actor-critic algorithm employs two neural networks: one for value-function approximation and another for policy. However, the key distinction lies in the actor-critic's use of the same network to estimate both the baseline and the return, as opposed to Vanilla PG, where the value network serves only as a baseline. The policy network, often referred to as "the actor," interacts with the environment, while the value network, known as "the critic," maintains the value of the actions taken by the actor.

\subsection{Actor-Critic Derivation}

The policy gradient expression
\begin{align*}
    \nabla_\theta J(\theta) = \BBE_{\tau \sim p_\theta} \left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) G_t\right],
\end{align*}
can be interpreted as a weighted version of the maximum likelihood objective, with the weights being the single-sample reward-to-go. In other words, policy gradient increases or decreases the likelihood of an action depending on the value of its single-sample reward-to-go. Compare this to behavioral cloning, where the likelihood of all actions in a given dataset is increased irrespective of their corresponding reward-to-go:
\begin{align*}
    \nabla_\theta J_{\rm {BC}}(\theta) = \BBE_{\tau \sim p_\beta} \left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t)\right].
\end{align*}
However, one way to reduce the variance in policy gradient is to replace the single-sample reward-to-go with the expected return given the state and action. That is

\begin{align*}
    \nabla_\theta J(\theta) &= \BBE_{\tau \sim p_\theta} \left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) \BBE_\pi[G_t \vert s_t, a_t]\right] \\
    &= \BBE_{\tau \sim p_\theta} \left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) Q^\pi(s_t,a_t)\right]\\
    &= \BBE_{\tau \sim p_\theta} \left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) \left[R(s_t, a_t)+ \BBE_{s_{t+1}\sim P(s_{t+1}\vert s_t, a_t)}[V^\pi(s_{t+1})]\right]\right].
\end{align*}
Now we make an approximation and we replace $\BBE_{s_{t+1}\sim P(s_{t+1}\vert s_t, a_t)}[V^\pi(s_{t+1})]$ by $V^\pi(s_{t+1})$, meaning that we consider a single-sample estimate for $V^\pi(s_{t+1})$. Note, however, that we are making the single-sample estimate only for one timestep. Since, after one timestep, at $s_{t+1}$, since we are using $V^\pi(s_{t+1})$, we are going to be under $\BBE_\pi$ thereafter. \textbf{This will introduce bias at the cost of lower variance}. The modified policy gradient then becomes
\begin{align*}
    \nabla_\theta J(\theta) &= \BBE_{\tau \sim p_\theta} \left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) \left[R(s_t, a_t)+ V^\pi(s_{t+1})\right]\right].
\end{align*}

To obtain the actor-critic method, we introduce the state-base baseline and discount factor, $V^\pi(s_t)$ and $\gamma$ and we define the estimate of the advantage at time step $t$, denoted as $\hat{A}_t$, as

$$\hat{A}_t = \underbrace{R(s_t,a_t) + \gamma V_\phi^\pi(s_{t+1})}_{\text{return estimator}} - \underbrace{V_\phi^\pi(s_t)}_{\text{baseline}},$$
and
\begin{align*}
    \nabla_\theta J(\theta) &= \BBE_{\tau \sim p_\theta} \left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) \hat A_t \right].
\end{align*}

This forms the basis of the online actor-critic algorithm.

\begin{figure}[ht]
\center{\includegraphics[width=0.7\textwidth] {Online_actor_critic}}
\caption{Online Actor-Critic Algorithm}\label{fig:onlice_actor_critic}
\end{figure} 
% 
\begin{center}
\begin{tcolorbox}
[breakable,enhanced,colback=purple!10!white,colframe=purple!30!black,
title={Batch actor-critic algorithm},width=0.7\linewidth]
\begin{enumerate}[\hspace{1cm}(1)]
    \item Sample $\{s_i,a_i,s_i',r_i\}$ under $\pi_\theta(a|s)$ (run the agent on the environment)
    \item Fit $\hat{V}_\phi^\pi(s)$ to sampled reward sum or TD
    \item Evaluate $\hat{A}^\pi(s_i,a_i) = r(s_i,a_i) + \hat{V}_\phi^\pi(s_i') - \hat{V}_\phi^\pi(s_i)$
    \item Update $\theta$ using $\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log\pi_\theta(a_i|s_i)\hat{A}^\pi(s_i,a_i)$
    \item Go back to step 1
\end{enumerate}
\end{tcolorbox}
\end{center}

This represents the Batch actor-critic algorithm, where the agent collects a batch of samples from the environment, updates the value function approximation, evaluates the advantage function, and performs a policy update using the policy gradient. The process is repeated iteratively to improve the policy and value function estimates.

\subsection{Discount Factor}
The discount factor introduces changes to the original MDP (with the transition dynamics $p(s^\prime\vert s, a)$), altering it into another MDP depicted in Figure~\ref{fig:discount_factor}. This modified MDP incorporates a "death" state, denoted as $s_\rmd$, which corresponds to adjusted transition dynamics $\tilde p(s^\prime\vert s, a)$ defined as:
\begin{align*}
\tilde p(s^\prime \vert s,a) = 
    \begin{cases}
        1 - \gamma, \qquad \qquad &\mbox{if } s^\prime = s_\rmd\\
        \gamma p(s^\prime \vert s, a), \qquad\qquad &\mbox{else.} 
    \end{cases}
\end{align*}
Given this adjusted MDP (Figure~\ref{fig:discount_factor}), we express the value function $V_\pi(s)$ as follows:
\begin{align*}
    V_\pi(s) &= \BBE_{a\sim\pi(a\vert s)}\left[ r(s,a) + \BBE_{s^\prime \sim \tilde p(s^\prime \vert s, a)}[V_\pi(s^\prime)]\right]\\
    &= \BBE_{a\sim\pi(a\vert s)}\left[ r(s,a) + \gamma \BBE_{s^\prime \sim p(s^\prime \vert s, a)}[V_\pi(s^\prime)] + (1 - \gamma)(\underbrace{V_\pi(s_\rmd)}_{=0}) \right]\\
    &= \BBE_{a\sim\pi(a\vert s)}\left[ r(s,a) + \gamma\BBE_{s^\prime \sim p(s^\prime \vert s, a)}[ V_\pi(s^\prime)]\right]
\end{align*}
The policy gradient for this modified MDP, after applying the causality trick, can be approximated as
\begin{align}
\nabla_\theta J(\theta) &\approx \BBE_{\tau\sim p_\theta}\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) \left(\sum_{t^\prime = t}^{T-1} \gamma ^{t^\prime}r(s_t, a_t)\right) \right]\nn\\
&= \BBE_{\tau\sim p_\theta}\left[\sum_{t=0}^{T-1} \gamma ^{t }\nabla_\theta \log \pi_\theta(a_t \vert s_t) \left(\sum_{t^\prime = t}^{T-1} \gamma ^{t^\prime -t}r(s_t, a_t)\right) \right]\label{eq:discount_factor_pg_1}
\end{align}
This is similar to the~\eqref{eq:off_policy_pg_after_causality2} with $\beta = \theta$. The interpretation of Equation~\eqref{eq:discount_factor_pg_1} is that the introduction of the death state (due to the discount factor) results in reduced importance assigned to both future rewards and future decisions. In essence, making the correct decision sooner becomes more critical, and the impact of $\nabla_\theta \log \pi_\theta(a_t \vert s_t)$ diminishes as $t$ increases.

However, in practice, we commonly omit the $\gamma^t$ term, leading to the use of the following policy gradient approximation:

\begin{align}
\nabla_\theta J(\theta) &\approx \BBE_{\tau\sim p_\theta}\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \vert s_t) \left(\sum_{t^\prime = t}^{T-1} \gamma ^{t^\prime -t}r(s_t, a_t)\right) \right]\label{eq:discount_factor_pg_2}
\end{align}

Based on Equation~\eqref{eq:discount_factor_pg_2}, the sole alteration in the actor-critic formulation lies in the computation of $\hat{A}^\pi(s,a)$, which is expressed as:

\begin{align*}
    \hat{A}^\pi(s,a) = r(s,a) + \gamma \hat{V}_\phi^\pi(s^\prime) - \hat{V}_\phi^\pi(s_i)
\end{align*}

\begin{figure}[ht]
\center{\includegraphics[width=0.3\textwidth] {Discount_Factor}}
\caption{MDP with Discount Factor}\label{fig:discount_factor}
\end{figure}

\subsection{Off-Policy Actor-Critic}
\begin{itemize}
    \item Transitions ($s_i, a_i, r_i, s^\prime_i)$ under any policy is stored in the replay buffer. Define $a_i^\pi \triangleq \pi_\theta(s_i)$.
    \item On-policy actor-critic has two limitations for being used as an off-policy algorithm:
    \begin{enumerate}
        \item In the actor-critic derivation, we replaced $\BBE_\pi[V^\pi(s^\prime)]$ with the single-sample estimate $V^\pi(s^\prime)$. This replacement was sort of valid as long as we at least sample under the same policy. Thus, we cannot use the transition $(s,a,s^\prime, r)$ from the replay buffer which was collected under another policy to update the value function (the critic update).
        However, if we instead use $Q$ values. Then, we can use the transition $(s,a,s^\prime, r)$ to update the $\hat Q_\phi^\pi$ values.        
        \item For the same reason, during the actor update, we are interested in updating the chance of selecting an action, based on the value of that action. Thus, we require to update the action that current policy would have chosen at state $s$, which is $\pi_\theta(s)$, and not the action $a$ from the transition.
    \end{enumerate}
    
    \item To train the actor we completely ignore the transition from the buffer, we only use $s_i$. The reason is that the reward $r_i$ in the buffer is the reward collected under $a_i$. However, the current policy takes action $a_i^\pi$, for which we don't know the reward to be collected. However, we can estimate the reward-to-go using the critic network $\hat Q_\phi^\pi$. We want to know how good is the action that we would have taken under current policy if we were at $s_i$.
    \item Note that we removed the baseline from the Actor Target. 
    \end{itemize}

\begin{figure}[ht]
\center{\includegraphics[width=0.9\textwidth] {Off_policy_Actor_critic}}
\caption{Off-Policy Actor-Critic Training}\label{fig:off_policy_actor_critic}
\end{figure} 


\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{2}
    \begin{tabular}{c|c|c}        
         & \textbf{On-Policy} & \textbf{Off-Policy} \\        
         \hline
        \textbf{Networks} & $\hat V_\phi^\pi, \pi_\theta$& $\hat Q_\phi^\pi, \pi_\theta$ \\        
        \textbf{Critic Target} & $\hat V_\phi^\pi(s_i) \gets r_i + \gamma \hat V_\phi^\pi(s^\prime_i)$ &  $\hat Q_\phi^\pi(s_i, a_i) \gets r_i + \gamma \hat Q_\phi^\pi(s^\prime_i, a_i^{\prime\pi})$\\       
        % \textbf{Actor Target} & $r_i + \gamma \hat V_\phi^\pi (s_i^\prime) - \hat V_\phi^\pi(s_i)$ & $\hat Q_\phi^\pi(s_i, a_i^\pi) -\hat V_\phi^\pi(s_i)$\\
        \textbf{Actor Grad} & $\frac{1}{N}\sum_i \nabla_\theta \log \pi_\theta(a_i \vert s_i) \hat A_\phi^\pi(s_i, a_i)$ & $\frac{1}{N}\sum_i \nabla_\theta \log \pi_\theta(a_i^\pi \vert s_i) \hat Q_\phi^\pi(s_i, a_i^\pi)$ \\        
    \end{tabular}
    \caption{On-Policy vs. Off-Policy Actor-Critic}
    \label{tab:my_table}
\end{table}


\subsection{Actor-critic implementation notes}

\begin{itemize}
\item In theory, we should perform value function regression every time we update our policy to match the new policy's behavior. However, in practice, this can be computationally expensive. Therefore, we may instead take a few gradient steps at each iteration.
\item Since our target values are based on the old value function, we need to update the targets (i.e., $r + \gamma V(s')$) with the updated value function. This can be done by following these steps:
\begin{enumerate}
\item Update targets with current value function.
\item Regress onto targets to update the value function by taking a few gradient steps.
\item Repeat steps 1 and 2 several times.
\end{enumerate}
\item The process of fitting the value function critic is an iterative process where we alternate between computing target values and updating the value function to match the targets. This iterative process is crucial for training the critic network.
\item In regular policy gradient methods, it is necessary to visit a state multiple times to have a good estimate of the return. However, in actor-critic methods, even if the exact same state is not visited multiple times, there may be other similar states visited in different trials. As a result, the function approximator can capture some shared information between these states, leading to a better estimate of the expected return compared to a single-sample Monte Carlo update. However, this estimate is not as accurate as repeatedly visiting the exact same state and taking multiple trials from it, but it still provides some benefit.
\end{itemize}

\newpage

\section{On-Policy Algorithms}
\subsection{GAE: Generalized Advantage Estimation}\label{sec:GAE}
\href{https://arxiv.org/abs/1506.02438}{\underline{Link to paper}} | \href{https://arxiv.org/abs/1506.02438}{\underline{Link to code ADD}} 

\vspace{10pt}
\subsubsection{Preliminaries}
Consider the definitions in~\Cref{fig:gae_summary}, and the following definition.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/GAE_table.png}
    \caption{Advantage functions definitions and their associated policy gradients}
    \label{fig:gae_summary}
\end{figure}

\begin{definition} 
The estimator $\hat{A}_t$ is $\gamma$-just if
\begin{align*}
\mathbb{E}_{s_{0:\infty} a_{0:\infty}} [\hat{A}_t(s_{0:\infty}, a_{0:\infty}) \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) ] &= \mathbb{E}_{s_{0:\infty} a_{0:\infty}} [A_{\pi,\gamma}(s_t, a_t) \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)].
\end{align*}
\end{definition}

A $\gamma$-just advantage function estimator $\hat{A}$ has the property that when it is substituted in place of $A^{\pi,\gamma}$ in the policy gradient, it does not introduce any additional bias.

With these definitions, we can present \Cref{prop:GAE_prop}, which give a sufficient condition for $\hat A$ to be $\gamma$-just.

\begin{proposition}\label{prop:GAE_prop}
    Suppose that $\hat A_t$ can be written in the form $\hat A_t(s_{0:\infty},a_{0:\infty}) = Q_t(s_{t:\infty},a_{t:\infty}) - b_t(s_{0:t},a_{0:t-1})$ such that for all $(s_t,a_t)$, $\BBE_{s_{t+1:\infty}, a_{t+1:\infty}\vert {s_t, a_t}}[Q_t(s_{t:\infty},a_{t:\infty})] = Q^{\pi, \gamma}(s_t, a_t)$. Then, $\hat A$ is $\gamma$-just.
\end{proposition}
See \cite{schulman2015high} for the proof.

\Cref{prop:GAE_prop} shows that an advantage estimator $\hat{A}_t$ will be $\gamma$-just if it is composed of two terms - a state-dependent baseline $b_t$ and a $\gamma$-discounted Q-function estimator $Q_t$. Critically, $b_t$ can only depend on past and current states and past actions while $Q_t$ can depend on the full trajectory.

This implies we can reduce policy gradient variance by subtracting any function of past and current states and past actions as a baseline, without introducing bias.


\subsubsection{Objectives and Approach}

\noindent \textbf{Objectives}:

\begin{itemize}
\item Provide an intuition for a variance reduction scheme in policy gradients.

\item Introduce an advantage function estimator that allows tuning the bias-variance tradeoff using two parameters $\gamma$ and $\lambda$.
\end{itemize}

\noindent \textbf{Approach}:

The GAE aims to find a lower variance approximation to the true advantage function $A^{\pi}$. It does this in two main ways:

\begin{itemize}
\item Introducing a discount factor $\gamma$ decreases variance at the cost of some bias. The GAE tries to estimate the $\gamma$-discounted advantage $A^{\pi,\gamma}$.

\item The parameter $\lambda$ controls the bias-variance tradeoff in the GAE estimator. Setting $\lambda=1$ gives an unbiased estimate of $A^{\pi,\gamma}$, while lower $\lambda$ reduces variance further but introduces some bias.
\end{itemize}

In summary, the GAE provides an advantage function estimator with two knobs - $\gamma$ and $\lambda$ - to tune the bias-variance tradeoff. This enables constructing policy gradient algorithms with reduced variance.
In this method,
\begin{itemize}
    \item GAE uses $V_\phi^{\pi, \gamma}$, the value function estimator, instead of the ground-truth $V^{\pi,\gamma}$
    \item $\gamma$ controls the bias in $A^{\pi, \gamma}$ with respect to $A^\pi$. $A^{\pi, \gamma}$ is an unbiased estimator of $A^\pi$ for $\gamma =1$. Even with an accurate value function, setting $\gamma < 1$ introduces bias into the estimate. 
    \item $\lambda$ controls the bias in $\hat A_t^{\rm{GAE}(\gamma, \lambda)}$ with respect to $A^{\pi, \gamma}$. $\hat A_t^{\rm{GAE}(\gamma, \lambda)}$ is an unbiased estimator of $A^{\pi, \gamma}$ for $\lambda = 1$, regardless of the accuracy of the function approximator (will be described). In contrast, $\lambda < 1$ introduces bias only when the value function is inaccurate.
    \item Empirically, the paper suggests that the optimal value for $\lambda$ is much lower than the optimal value for $\gamma$. This difference is likely because $\lambda$ introduces less bias than $\gamma$, especially when the value function is reasonably accurate.
    \item Increasing either $\lambda$ or $\gamma$ reduces bias, but they do so toward different targets, and this reduction in bias comes at the cost of increased variance.
\end{itemize}

\subsubsection{GAE Advantage Function}

GAE advantage function is equivalent to TD($\lambda$), which is the weighted-sum $n$-step TD errors, weighted by, $\lambda^n$, with $V_\phi^{\pi, \gamma}$ being the value function approximator. Consider the following defintions:
\begin{align*}
    \delta_t &\triangleq r_t + \gamma V_\phi^{\pi, \gamma}(s_{t+1}) - V_\phi^{\pi, \gamma}(s_t),\\
    \hat A_t^{(k)} &\triangleq \sum_{\ell=1}^{k-1} \gamma^{\ell} r_{t+\ell} + \gamma^k V_\phi^{\pi, \gamma}(s_{t+k}) - V_\phi^{\pi, \gamma}(s_t) = \sum_{\ell =0}^{k-1} \gamma^\ell \delta_{t+\ell}.\\
    % \left( \hat A_t^{(1)} + \lambda \hat A_t^{(2)} + \lambda^2 \hat A_t^{(3)} + \cdots \right)
\end{align*}

\noindent\textbf{Infinite-horizon Case}

\begin{align*}
    \hat A_t^{\rm{GAE}(\gamma,\lambda)} &\triangleq (1-\lambda) \sum_{\ell = 0}^\infty \lambda^{\ell} \hat A_t^{(\ell + 1)}  = \sum_{\ell = 0}^\infty (\gamma \lambda)^{\ell} \delta_{t+\ell}
\end{align*}
It can be shown from the above definition that the following holds:
\begin{align*}
     \hat A_t^{\rm{GAE}(\gamma,\lambda)} = \frac{1}{1-\lambda}\delta_t + \gamma \lambda \hat A_{t+1}^{\rm{GAE}(\gamma,\lambda)}
\end{align*}

Note that 
\begin{align*}
    \hat A_t^{\rm{GAE}(\gamma,1)} &= \sum_{\ell = 0}^\infty \gamma^\ell \delta_{t+\ell} = \sum_{\ell = 0}^\infty \gamma^\ell r_{t+\ell} - V_\pi^{\pi, \gamma}(s_t)\\
    \hat A_t^{\rm{GAE}(\gamma,0)} &= \delta_t
\end{align*}
Thus, $\rm{GAE}(\gamma, 1)$ is $\gamma$-just regardless of accuracy of $V_\phi^{\pi, \gamma}$.

\vspace{10pt}
\noindent\textbf{Finite-horizon Case}

\begin{align*}
\hat{A}_t^{\rm GAE} = \delta_t + (\gamma \lambda) \delta_{t+1} + (\gamma \lambda)^2 \delta_{t+2} + \ldots + (\gamma \lambda)^{T-t-1} \delta_{T-1},
\end{align*}
and
\begin{align*}
    \hat A_t^{\rm{GAE}(\gamma,\lambda)} &\triangleq \frac{1 - \lambda}{1 - \lambda^T} \sum_{\ell=0}^{T-1} \lambda^\ell \hat A_t^{(\ell+1)} = \frac{1}{1 - \lambda^T}\sum_{\ell=0}^{T-1} (\gamma \lambda)^\ell \left(1 - \lambda^{T-\ell}\right)\delta_{t+\ell}
\end{align*}

\newpage
\section{Off-Policy Algorithms}
\subsection{DDPG: Deep Deterministic Policy Gradient}\label{sec:DDPG}
\href{https://arxiv.org/abs/1509.02971}{\underline{Link to paper}} | \href{https://arxiv.org/abs/1509.02971}{\underline{Link to code ADD}} 

\subsubsection{Objectives and Approach}
\textbf{Objective}:
\begin{itemize}
\item Develop a reinforcement learning method for continuous control tasks in which an agent interacts with an environment to learn optimal control policies.
\item Address the challenge of high-dimensional action spaces and provide a solution for learning effective policies in such environments.
\end{itemize}

\noindent \textbf{Approach}:
\begin{itemize}
\item Utilize a Deep Deterministic Policy Gradient (DDPG) algorithm that combines deep neural networks for both the actor and critic networks, allowing for continuous action spaces and effective policy optimization.
\item Incorporate experience replay and target networks to stabilize and improve the training process, facilitating the efficient learning of complex control policies in continuous action spaces.
\end{itemize}
\subsubsection{DDPG Theory}
Let's consider the critic $Q^\mu_{\theta^Q}(s, a)$, which is parameterized by $\theta^Q$, and the deterministic actor $\mu_{\theta_\mu}$, parameterized by $\theta_\mu$. The training of the $Q^\mu_{\theta^Q}$ network follows the DQN paradigm as outlined in \cite{mnih2015human}. However, note that the DQN training process is off-policy in nature. Therefore, to construct a fully off-policy actor-critic method, we must have an actor that is trained using off-policy trajectories. To achieve this, we must derive the expression for the off-policy deterministic policy gradient (DPG). Before delving into these details, let's introduce the concept of on-policy DPG from \cite{silver2014deterministic}.

\vspace{10pt}
\noindent\textbf{On-Policy DPG}

\noindent The objective is defined as follows:
\begin{align}
J(\theta^\mu) \triangleq \BBE_{s_0\sim p_0} \left[V^\mu(s_0)\right]
\end{align}
This objective is analogous to the one used in stochastic policy gradient methods, with the key distinction that $\mu_{\theta^\mu}(s)$ is a deterministic policy. The on-policy deterministic policy gradient (DPG), as introduced by \cite{silver2014deterministic}, can be expressed as:

\begin{align*}
\nabla_{\theta^\mu} J(\theta^\mu) \triangleq \BBE_{s \sim d^\mu}\left[\nabla_{a} Q^\mu_{\theta^Q}(s,a)\vert_{a = \mu(s)} \nabla_{\theta^\mu}\mu(s) \right].
\end{align*}

The proof, which resembles the proof for the stochastic policy gradient but with the replacement of the stochastic policy $\pi(a\vert s)$ by a deterministic policy $\mu(s)$, can be found \href{http://proceedings.mlr.press/v32/silver14-supp.pdf}{\underline{here}}.

\vspace{10pt}
\noindent\textbf{Off-Policy DPG}

\noindent The objective is defined as follows:
\begin{align}\label{eq:off_policy_dpg_objective}
J^\beta(\theta^\mu) = \BBE_{s \sim d^\beta} \left[V^\mu(s)\right],
\end{align}
and the off-policy DPG is approximated as:
\begin{align}\label{eq:off_policy_dpg}
    \nabla_{\theta^\mu} J^\beta(\theta^\mu) \approx \BBE_{s \sim d^\beta} \Bigg[\nabla_{\theta^\mu} \mu(s) \nabla_a Q^\mu(s,a)\vert_{a=\mu(s)}\Bigg]
\end{align}

\begin{tcolorbox}[breakable,enhanced,colback=gray!10!white,colframe=gray!50!black,
title={Off-Policy Deterministic Policy Gradient (Off-Policy DPG)}]
First, we directly compute the gradient of~\cref{eq:off_policy_dpg_objective}, yielding:
\begin{align*}
    \nabla_{\theta^\mu} J^\beta(\theta^\mu) &= \BBE_{s \sim d^\beta} \left[\nabla_{\theta^\mu} V^\mu(s)\right] = \BBE_{s \sim d^\beta} \left[\nabla_{\theta^\mu} Q^\mu(s, \mu(s))\right]\\
    &=\BBE_{s \sim d^\beta} \left[\nabla_{\theta^\mu} \left[r(s, \mu(s)) + \gamma  \int_\SSS p(s^\prime\vert s, \mu(s)) V^\mu(s^\prime) \rmd s^\prime \right] \right]\\
    &= \BBE_{s \sim d^\beta} \Bigg[\nabla_{\theta^\mu} \mu(s) \nabla_a r(s,a)\vert_{a=\mu(s)} \\
    & \hphantom{=\BBE_{s \sim d^\beta}\Bigg[} + \gamma \int_\SSS\Big(p(s^\prime\vert s, \mu(s))\nabla_{\theta^\mu}V^\mu(s^\prime) + \nabla_{\theta^\mu} \mu(s) \nabla_a p(s^\prime\vert s, \mu(s))\vert_{a=\mu(s)} V^\mu(s^\prime) \Big)\rmd s^\prime \Bigg]\\
    &= \BBE_{s \sim d^\beta} \Bigg[\nabla_{\theta^\mu} \mu(s) \nabla_a \Bigg(r(s,a) + \gamma \int_\SSS p(s^\prime\vert s, \mu(s)) V^\mu(s^\prime)\rmd s^\prime\Bigg)\Bigg\vert_{a=\mu(s)}+ \gamma \int_\SSS p(s^\prime\vert s, \mu(s))\nabla_{\theta^\mu}V^\mu(s^\prime) \rmd s^\prime\Bigg]\\
    &= \BBE_{s \sim d^\beta} \Bigg[\nabla_{\theta^\mu} \mu(s) \nabla_a Q^\mu(s,a)\vert_{a=\mu(s)}+ \gamma \int_\SSS p(s^\prime\vert s, \mu(s))\nabla_{\theta^\mu}V^\mu(s^\prime) \rmd s^\prime\Bigg]
\end{align*}
In the on-policy case, we maintain the expansion of the $V^\mu(s^\prime)$ terms as they occur. In contrast, within the off-policy case, we introduce an approximation by considering only the initial term. As a result, the approximated policy gradient is formulated as~\eqref{eq:off_policy_dpg}
\end{tcolorbox}




\subsubsection{DDPG Algorithm}
\begin{algorithm}
\caption{DDPG algorithm}
\begin{algorithmic}[h!]
\State Randomly initialize critic network $Q(s,a|\theta^{Q})$ and actor $\mu(s|\theta^{\mu})$ with weights $\theta^{Q}$ and $\theta^{\mu}$
\State Initialize target network $Q'$ and $\mu'$ with weights $\theta^{Q'} \gets \theta^{Q}, \theta^{\mu'} \gets \theta^{\mu}$
\State Initialize replay buffer $\mathcal{R}$
\For{episode $= 1, M$}

\State Initialize a random process $\mathcal{N}$ for action exploration
\State Receive initial observation state $s_{1}$
\For{$t = 1, T$}
\State Select action $a_{t} = \mu(s_{t}|\theta^{\mu}) + \mathcal{N}_{t}$ according to current policy and exploration noise
\State Execute action $a{t}$ and observe reward $r_{t}$ and new state $s_{t+1}$
\State Store transition $(s_{t}, a_{t}, r_{t}, s_{t+1})$ in $\mathcal{R}$
\State Sample random minibatch of $N$ transitions $(s_{i}, a_{i}, r_{i}, s_{i+1})$ from $\mathcal{R}$
\State Set $y_{i} = r_{i} + \gamma Q'(s_{i+1}, \mu'(s_{i+1}|\theta^{\mu'})|\theta^{Q'})$
\State Update critic by minimizing the loss: $L = \frac{1}{N}\sum_{i}(y_{i} - Q(s_{i}, a_{i}|\theta^{Q}))^2$
\State Update the actor policy using sampled policy gradient: $$\nabla_{\theta^{\mu}} J \approx \frac{1}{N}\sum_{i} \nabla_{a}Q(s, a|\theta^{Q})|_{{s=s_{i},a=\mu(s_{i})}} \nabla_{\theta^{\mu}} \mu(s|\theta^{\mu})|_{s_{i}}$$

\State Update target networks:
$$\theta^{Q'} \gets \tau \theta^{Q} + (1 - \tau)\theta^{Q'}$$
$$\theta^{\mu'} \gets \tau \theta^{\mu} + (1 - \tau)\theta^{\mu'}$$
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\subsubsection{Stabilizing Learning}
DDPG makes several tweaks to stabilize training with neural networks:

\begin{itemize}
\item \textbf{Use replay buffer}: Store transitions in a replay buffer. Sample mini-batches for training.

\item \textbf{Use soft target update}: Use separate slowly-updating target networks $Q'$ and $\mu'$ to calculate the target Q values during Bellman updates. This provides more consistent targets during temporal difference learning.

\item \textbf{Use batch minimization}: To address varying units and ranges in low-dimensional feature vectors, batch normalization is applied—a deep learning technique normalizing each dimension in minibatches to maintain consistency across environments and unit scales, reducing manual scaling needs. It's used on the state input and all layers of $\mu$-network and $Q$-network before the action input, facilitating effective learning across diverse tasks with different unit types without manual adjustments. This technique also mitigates covariance shift during training.

\item \textbf{Use noise for the policy}: Add temporally correlated noise to the actor policy during exploration to facilitate efficient exploration in environments with momentum. For example, use an Ornstein-Uhlenbeck process rather than white noise.
\end{itemize}

\subsubsection{Network Specifications}
\begin{table}[h!]
\centering
\begin{tabular}{l|l}
\textbf{Parameter/Specification} & \textbf{Value} \\
\hline
Optimizer & Adam \\
Learning Rate (Actor) & $10^{-4}$ \\
Learning Rate (Critic) & $10^{-3}$ \\
L2 Weight Decay (Q) & $10^{-2}$ \\
Discount Factor ($\gamma$) & 0.99 \\
Soft Target Update ($\tau$) & 0.001 \\
Activation Function & Rectified Non-linearity \\
Final Output Layer (Actor) & tanh \\
Hidden Layers (Low-dimensional Networks) & 400 units, 300 units \\
Actions Included & 2nd Hidden Layer of Q \\
Minibatch Size (Low-dimensional) & 64 \\
Replay Buffer Size & $10^6$ \\
\end{tabular}
\caption{Neural Network Training Parameters and Specifications}
\end{table}

\subsection{SAC: Soft Actor-Critic}
The notion of entropy

The entropy of the unknown result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal probability 1/2). This is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss; the result of each toss of the coin delivers one full bit of information. This is because
\href{https://en.wikipedia.org/wiki/Entropy_(information_theory)#:~:text=The%20entropy%20of%20the%20unknown,uncertainty%20and%20therefore%20maximum%20entropy
}{See example}

\newpage
\section{Appendix: Basic Statistics}\label{apx:basic_statistics}

\begin{itemize}
  \item \textbf{Probability:}
  \begin{itemize}
    \item $P(X)$ represents the probability of event $X$, ranging from 0 to 1.
    \item $X'$ denotes the complement of event $X$, such that $P(X') = 1 - P(X)$.
    \item $X \cup Y$ signifies the union of events $X$ and $Y$, with $P(X \cup Y) = P(X) + P(Y) - P(X \cap Y)$.
  \end{itemize}

  \item \textbf{Probability Properties:}
  \begin{itemize}
    \item \textbf{Marginal Probability:}
    \begin{itemize}
      \item For joint probability distribution $P(X, Y)$, the marginal probability of $X$ is obtained by summing or integrating over all possible values of $Y$: $P(X) = \int P(X, y) dy$.
    \end{itemize}
    \item \textbf{Conditional Probability:}
    \begin{itemize}
      \item $P(X|Y) = \frac{P(X \cap Y)}{P(Y)}$ denotes the conditional probability of $X$ given $Y$.
    \end{itemize}
    \item \textbf{Independence:}
    \begin{itemize}
      \item Events $X$ and $Y$ are independent if $P(X \cap Y) = P(X) \cdot P(Y)$.
    \end{itemize}
    \item \textbf{Law of Total Probability:}
    \begin{itemize}
      \item $P(A) = \sum_{i} P(A|B_i) \cdot P(B_i)$ for mutually exclusive and exhaustive events $B_1, B_2, \ldots$.
    \end{itemize}
  \end{itemize}

  \item \textbf{Expectation:}
  \begin{itemize}
    \item $\BBE(X)$ or $\mu$ represents the expectation of random variable $X$.
    \begin{itemize}
      \item For discrete $X$: $\BBE(X) = \sum_{x} x \cdot P(X = x)$.
      \item For continuous $X$ with probability density function $f(x)$: $\BBE(X) = \int x \cdot f(x) dx$.
    \end{itemize}
    \item \textbf{Linearity of Expectation:}
    \begin{itemize}
      \item $\BBE(aX + bY) = a\BBE(X) + b\BBE(Y)$ for random variables $X$ and $Y$, and constants $a$ and $b$.
    \end{itemize}
    \item \textbf{Law of the Unconscious Statistician:}
    \begin{itemize}
      \item $\BBE(g(X)) = \sum_{x} g(x)P(X=x)$, where $P(X=x)$ is the probability mass function of $X$.
    \end{itemize}
    \item \textbf{Importance Sampling:}
    \begin{itemize}
      \item $\BBE_{x\sim p(x)}[f(x)] = \BBE_{x\sim q(x)}\left[\frac{p(x)}{q(x)}f(x)\right]$.
      
    \end{itemize}
    
  \end{itemize}

  \item \textbf{Variance:}
    \begin{itemize}
    \item The variance of random variable $X$, denoted as $\text{Var}(X)$ or $\sigma^2$, measures the spread of $X$ around its expected value:
    \[
    \text{Var}(X) = E[(X - E(X))^2] = E(X^2) - [E(X)]^2
    \]    

        \item \textbf{Covariance:}
        \begin{itemize}
            \item $\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]$ measures the linear relationship between random variables $X$ and $Y$.
        \end{itemize}
        \item \textbf{Conditional Expectation:}
        \begin{itemize}
            \item $\BBE(X|Y=y) = \sum_{x} x \cdot P(X = x|Y = y)$ for discrete $X$ and $Y$, and $\BBE(X|Y=y) = \int x \cdot f(x|y) dx$ for continuous $X$ and $Y$ with conditional probability density function $f(x|y)$.
        \end{itemize}        
        \item \textbf{Law of Iterated Expectations:} 
        \begin{itemize}
            \item $\BBE(E(X|Y)) = E(X)$.
        \end{itemize}

    
    \end{itemize}
    

\end{itemize}
% \section{Maximum Entropy Method in Reinforcement Learning}

% The Maximum Entropy Method (MaxEnt) is an important framework in reinforcement learning that extends the traditional reinforcement learning setup by considering the concept of entropy. It's particularly useful in cases where you want to find a stochastic policy that not only maximizes expected rewards but also has a certain level of randomness or entropy. This can be helpful in various scenarios, such as in exploration or when dealing with partially observable environments.

% \subsection{Markov Decision Process (MDP)}

% At the core of reinforcement learning, there's an environment modeled as a Markov Decision Process. It's defined by:

% \begin{itemize}
%   \item A set of states ($S$): The possible situations or configurations of the environment.
%   \item A set of actions ($A$): The possible actions the agent can take.
%   \item Transition dynamics ($P$): The probability of transitioning from one state to another after taking a particular action.
%   \item Reward function ($R$): The immediate reward received after taking a specific action in a given state.
%   \item Discount factor ($\gamma$): A parameter that determines the agent's preference for immediate rewards over future rewards.
% \end{itemize}

% \subsection{Policy ($\pi$)}

% A policy is a strategy that the agent uses to select actions in various states. It's typically represented as a probability distribution over actions for each state: $\pi(a|s)$ - the probability of taking action 'a' in state 's'.

% In traditional RL, the objective is often to find a policy that maximizes the expected cumulative reward (return) over time.

% \subsection{Entropy ($H$)}

% Entropy is a measure of the randomness or uncertainty in a probability distribution. In the context of RL, it quantifies how 'random' the policy is. The entropy of a policy $\pi$ is defined as:

% \[
% H(\pi) = -\sum_{s}\sum_{a} \pi(a|s) \cdot \log(\pi(a|s))
% \]

% Intuitively, higher entropy policies are more exploratory and distribute their probability mass more evenly across actions.

% \subsection{Objective Function with Maximum Entropy}

% The goal in Maximum Entropy Reinforcement Learning is to find a policy that not only maximizes expected rewards but also maximizes entropy. This is done by optimizing the following objective function:

% \[
% J(\pi) = \sum_{s}\sum_{a} \pi(a|s) \cdot [Q(s, a) - \alpha \cdot H(\pi)]
% \]

% Here, $Q(s, a)$ represents the expected cumulative reward of taking action 'a' in state 's', which can be estimated using techniques like Q-learning or policy gradient methods. $\alpha$ is a hyperparameter that controls the trade-off between maximizing rewards and maximizing entropy. A higher $\alpha$ encourages more exploratory behavior.

% \subsection{Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL)}

% Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) is an extension of the MaxEnt framework used for solving the inverse reinforcement learning problem. In MaxEnt IRL, the goal is to infer the underlying reward function of an expert demonstrator from observed behavior. The key idea is to find a reward function such that the expert's behavior is optimal under the derived policy.

% \subsubsection{Objective Function in MaxEnt IRL}

% In MaxEnt IRL, the objective function to be optimized is as follows:

% \[
% \begin{aligned}
% & \max_{R} \sum_{t=0}^{T}\left(\sum_{a}\pi^{\ast}(a|s_t) \cdot R(s_t, a) - \alpha \cdot H(\pi^{\ast})\right) \\
% & \text{subject to:} \quad \pi^{\ast} = \text{argmax}_{\pi} \sum_{t=0}^{T}\left(\sum_{a}\pi(a|s_t) \cdot R(s_t, a) - \alpha \cdot H(\pi)\right)
% \end{aligned}
% \]

% Here, $R$ represents the reward function to be learned, $\pi^{\ast}$ is the expert's policy, and $\pi$ is the policy under consideration. The optimization seeks to find a reward function $R$ that aligns with the expert's behavior while maximizing entropy to allow for multiple solutions.

% \subsection{References}

% Inverse reinforcement learning aims to recover the reward function of an expert policy demonstrator. The maximum entropy principle proposes to find the reward function that maximizes the entropy of the induced trajectory distribution, subject to feature matching constraints.

% Let $S$ be the state space, $A$ the action space, $\rho_\pi(s)$ the discounted state visitation frequencies under policy $\pi$, and $f(s,a)$ the feature vector. The feature matching constraints are:
% \begin{align*}  
% \mathbb{E}_{\rho_{\pi_E}}[f(s,a)] = \mathbb{E}_{\rho_\pi}[f(s,a)],
% \end{align*}
% where $\pi_E$ is the expert policy.

% The trajectory distribution induced by a policy $\pi$ is $p_\pi(\tau) = p(s_0)\prod_{t=0}^{T-1} p(s_{t+1}|s_t,a_t)\pi(a_t|s_t)$. The entropy of this distribution is $H(\pi) = -\sum_\tau p_\pi(\tau)\log p_\pi(\tau)$.

% The maximum entropy inverse RL problem is:
% \begin{align*}
% &\max_\pi H(\pi)\\  
% &\text{s.t. } \mathbb{E}_{\rho_{\pi_E}}[f(s,a)] = \mathbb{E}_{\rho_\pi}[f(s,a)]
% \end{align*}

% The paper shows that the optimal $\pi^*$ is given by the maximum entropy distribution over trajectories
% \begin{align*}
% p^*(\tau) = \frac{1}{Z}\exp(\theta^T f(\tau)), 
% \end{align*}
% where $Z$ is the partition function and $\theta$ are the dual variables.

% The optimal policy $\pi^*$ is the maximum entropy policy induced by the reward function $R(s,a) = \theta^T f(s,a)$. The dual variables $\theta$ are found by maximizing the dual function 
% \begin{align*}
% g(\theta) = H(\pi^*) + \theta^T\mathbb{E}_{\rho_{\pi_E}}[f(s,a)].
% \end{align*}

% Let me know if you need any clarification or have additional questions!

% \begin{enumerate}
%   \item Ziebart, B. D., Maas, A. L., Bagnell, J. A., & Dey, A. K. (2008). Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd national conference on artificial intelligence (pp. 1433-1438).
%   \item Haarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017). Reinforcement learning with deep energy-based policies. In Proceedings of the 34th International Conference on Machine Learning.
% \end{enumerate}

% \end{document}

\newpage
\bibliographystyle{elsarticle-num} 
\bibliography{policy_gradient} 

\end{document}